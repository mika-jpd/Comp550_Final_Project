{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mikas_experimentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoqZoi1T0rkg"
      },
      "outputs": [],
      "source": [
        "#Mika's part for experimentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import torch\n",
        "\n",
        "#plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "#PyTorch imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score\n",
        "import torchtext\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n"
      ],
      "metadata": {
        "id": "pBhfCxW2CgCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSDSXwu9OSG_",
        "outputId": "d686df91-fe9c-4a79-9768-d6dd1aee7b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the data\n"
      ],
      "metadata": {
        "id": "Z1xLccUXBap_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data preparation\n",
        "path_data_sexist_comments = \"/content/drive/My Drive/Comp550 Final Project/ISEP Sexist Data labeling.xlsx\"\n",
        "path_data_amazon = \"/content/drive/MyDrive/COMP550 Final Project/AmazonMini.csv\"\n",
        "path_data_imdb = \"/content/drive/MyDrive/COMP550 Final Project/IMDB.csv\"\n",
        "\n",
        "#CHANGE to pd.read_csv FOR OTHER DATASETS :))\n",
        "df_sexist = pd.read_excel(path_data_sexist_comments)\n",
        "df_amazon = pd.read_csv(path_data_amazon)\n",
        "df_imdb = pd.read_csv(path_data_imdb)\n",
        "\n",
        "#path to Glove\n",
        "path_glove = \"/content/drive/MyDrive/Comp550 Final Project/glove.6B.50d.txt\""
      ],
      "metadata": {
        "id": "tfJhoB1QB9U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing of the sexist comments dataset"
      ],
      "metadata": {
        "id": "9jxW1jlzwQ2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-process the sexist comment dataset\n",
        "def lowerCase(StringArray):\n",
        "    lowerArray = []\n",
        "    for i in StringArray:\n",
        "        lowerArray.append(i.lower())\n",
        "    return lowerArray\n",
        "\n",
        "def depunctuate(StringArray):\n",
        "    exclude = set(string.punctuation)\n",
        "    depStringArray = []\n",
        "    for i in StringArray:\n",
        "        s = ''.join(ch for ch in i if ch not in exclude)\n",
        "        depStringArray.append(s)\n",
        "    return depStringArray\n",
        "\n",
        "data_sexist_sentences = df_sexist['Sentences'].to_numpy()\n",
        "\n",
        "#lowercase the sentences\n",
        "data_sexist_sentences = lowerCase(data_sexist_sentences)\n",
        "\n",
        "#depunctuate\n",
        "data_sexist_sentences = depunctuate(data_sexist_sentences)\n",
        "\n",
        "#load the final data\n",
        "df_sexist['Sentences'] = pd.Series(data_sexist_sentences)"
      ],
      "metadata": {
        "id": "cAXZX96ANXNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create helper classes for the BiLSTM creation and the data loading"
      ],
      "metadata": {
        "id": "RNhaUcvCBkV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bi-LSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        #size of input vectors, that'll be the embedding size\n",
        "        self.hidden_dim = args['hidden_dim']\n",
        "\n",
        "        #number of words in the vocabulary\n",
        "        self.vocab_size = args['vocab_size']\n",
        "\n",
        "        #number of classes you're predicting (usually binary)\n",
        "        self.output_dim = args['output_dim']\n",
        "\n",
        "        #input size is the size of each word/embedding\n",
        "        self.input_size = args['input_size']\n",
        "\n",
        "        #Dropout\n",
        "        self.drp = nn.Dropout(args['drp'])\n",
        "\n",
        "        #Embedding layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.input_size)\n",
        "        self.embedding.weight=nn.Parameter(torch.tensor(args['embedding_matrix'], dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad=args['requires_grad']\n",
        "\n",
        "        #Bi-LSTM\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
        "                            hidden_size=self.hidden_dim,\n",
        "                            dropout = args['drp'],\n",
        "                            num_layers=2,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        self.linear = nn.Linear(self.hidden_dim*2, self.output_dim)\n",
        "\n",
        "        #Softmax activation function\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    #Here x is the input tensor with shape: [batch_size, sequence_length]\n",
        "    #ie. number of training examples * number of words/training examples\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        output, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        h_n = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
        "\n",
        "        x = self.linear(h_n)\n",
        "\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = torch.squeeze(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4WaqLEJpLs-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, path_to_glove, embedding_dimension, max_nb_words=None, max_seq_length=None):\n",
        "        #here you could simply receive the dataset already with embedding matrix, index, and pre-processed text\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.df = df\n",
        "        self.path_to_glove = path_to_glove\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "\n",
        "        #get in _init_dataset\n",
        "        self.length = None\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "        #None by default\n",
        "        self.max_nb_words = max_nb_words\n",
        "        self.max_seq_length=max_seq_length\n",
        "\n",
        "        #initialize with method\n",
        "        self._init_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    #for a\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "    #input np.array of sentences\n",
        "    def preprocessing(self, np_dataset):\n",
        "        return np_dataset\n",
        "\n",
        "    def create_embedding_matrix(self, word_index, embedding_dict):\n",
        "        embedding_m = np.zeros((len(word_index) + 1, self.embedding_dimension))\n",
        "        for word, index in word_index.items():\n",
        "            if word in embedding_dict:\n",
        "                embedding_m[index] = embedding_dict[word]\n",
        "        self.embedding_matrix = embedding_m\n",
        "\n",
        "    def _init_dataset(self):\n",
        "        #create the array of labels\n",
        "        y = pd.Series.to_numpy(self.df['Label'])\n",
        "        for i in y:\n",
        "            self.labels.append(i)\n",
        "\n",
        "        #returns rows have individual sentences, non-tokenized!\n",
        "        X = self.preprocessing(pd.Series.to_numpy(self.df['Sentences']))\n",
        "\n",
        "        #create the word embeddings\n",
        "        embed_dict = {}\n",
        "        with open(path_glove, 'r', encoding=\"utf8\") as file:\n",
        "            Lines = file.readlines()\n",
        "\n",
        "            for line in Lines:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "\n",
        "                try:\n",
        "                    vector = np.asarray(values[1:], 'float32')\n",
        "                except:\n",
        "                    pass\n",
        "                embed_dict[word] = vector\n",
        "\n",
        "        if self.max_seq_length == None:\n",
        "            self.max_seq_length = max(len(i.split()) for i in X)\n",
        "\n",
        "        self.length = X.shape[0]\n",
        "\n",
        "        #tokenize the next and put in array of integers, where each integer is mapped to a word\n",
        "        np.random.seed(7)\n",
        "        if self.max_nb_words != None:\n",
        "            tokenizer = Tokenizer(num_words=self.max_nb_words)\n",
        "        else:\n",
        "            tokenizer = Tokenizer(num_words=len(embed_dict.keys()))\n",
        "\n",
        "        #maybe move this into another method for efficiency's sake\n",
        "        tokenizer.fit_on_texts(X)\n",
        "        sequences = tokenizer.texts_to_sequences(X)\n",
        "        word_index = tokenizer.word_index\n",
        "        text = pad_sequences(sequences, maxlen=self.max_seq_length)\n",
        "        indices = np.arange(self.length)\n",
        "        text = text[indices]\n",
        "\n",
        "        for i in text:\n",
        "            self.samples.append(i)\n",
        "        self.create_embedding_matrix(word_index=word_index, embedding_dict=embed_dict)"
      ],
      "metadata": {
        "id": "wCdzpPVYLyWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create the data loaders and the BiLSTM model"
      ],
      "metadata": {
        "id": "rTIMHw8VCMxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load into validation and training dataset\n",
        "#SEXIST\n",
        "dataset_sexist = CustomDataset(df= df_sexist, path_to_glove=path_glove, embedding_dimension=50)\n",
        "trainset_sexist, valset_sexist = random_split(dataset_sexist, [int(np.floor(len(dataset_sexist) * 0.8)), int(np.ceil(len(dataset_sexist) * 0.2))])\n",
        "\n",
        "train_loader_sexist = DataLoader(trainset_sexist, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader_sexist = DataLoader(valset_sexist, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "#AMAZON\n",
        "dataset_amazon = CustomDataset(df= df_amazon, path_to_glove=path_glove, embedding_dimension=50)\n",
        "trainset_amazon, valset_amazon = random_split(dataset_amazon, [int(np.floor(len(dataset_amazon) * 0.7)), int(np.ceil(len(dataset_amazon) * 0.3))])\n",
        "\n",
        "train_loader_amazon = DataLoader(trainset_amazon, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader_amazon = DataLoader(valset_amazon, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "#IMDB\n",
        "dataset_imdb = CustomDataset(df= df_imdb, path_to_glove=path_glove, embedding_dimension=50)\n",
        "trainset_imdb, valset_imdb = random_split(dataset_imdb, [int(np.floor(len(dataset_imdb) * 0.7)), int(np.ceil(len(dataset_imdb) * 0.3))])\n",
        "\n",
        "train_loader_imdb = DataLoader(trainset_imdb, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader_imdb = DataLoader(valset_imdb, batch_size=128, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "MHjJxNuZL6AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fill the args dictionary with the required parameters\n",
        "args = {\n",
        "    'hidden_dim' : 32, #number of hidden dim\n",
        "    'vocab_size' : dataset_sexist.embedding_matrix.shape[0], #number of rows in word embedding matrix\n",
        "    'input_size' : 50,\n",
        "    'output_dim' : 1, #number of classes you're predicting\n",
        "    'embedding_matrix' : dataset_sexist.embedding_matrix, #the embedding matrix you've built in dataset constructor\n",
        "    'drp' : 0.2, #dropout layer for forward LSTM\n",
        "    'requires_grad': False,\n",
        "}"
      ],
      "metadata": {
        "id": "GQAURpNyL8tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we train the BiLSTM model with 5 epochs. We will do this ten times and then average the model's performance on the validation dataset with every new batch of batch_size points that it sees. We use the average of the ten runs to plot the score in order to get a true idea of what the model's scores are on the validation data "
      ],
      "metadata": {
        "id": "bhwWRTkOi74-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_vanilla = {\n",
        "    'accuracy_training': [],\n",
        "    'loss': [],\n",
        "    'accuracy_validation': []\n",
        "}\n",
        "\n",
        "model = BiLSTM(args)"
      ],
      "metadata": {
        "id": "C9v1isPKLPqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(10):\n",
        "    model = BiLSTM(args)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    training_score = []\n",
        "    loss_score = []\n",
        "    validation_score = []\n",
        "    for e in range(5):\n",
        "        for i, data in enumerate(train_loader_sexist):\n",
        "            model.train()\n",
        "\n",
        "            running_loss = 0\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optim.Adam(model.parameters(), lr=0.01).zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            #Get the loss on the batch\n",
        "            targets = labels.to(torch.float32)\n",
        "            outputs = outputs.to(torch.float32)\n",
        "            \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            #gets the gradient on the batch for each parameter\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "            #take size lr step in gradient direction\n",
        "            optimizer.step()\n",
        "\n",
        "            #get the f1 score\n",
        "            f1_score_train = f1_score(targets.detach().numpy(), torch.round(outputs).detach().numpy())\n",
        "            #print(f'targets: {targets.detach().numpy()} {targets.shape} outputs: {torch.round(outputs).detach().numpy()} {outputs.shape}')\n",
        "            #print(f'f1_score: {f1_score_train}')\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            #validation\n",
        "            model.eval()\n",
        "\n",
        "            #set metrics to 0\n",
        "            total_f1_score_val = 0\n",
        "            count_val = 0\n",
        "\n",
        "            #Calculating the accuracy\n",
        "            model.eval()\n",
        "            for i_val, data_val in enumerate(val_loader_sexist):\n",
        "                inputs_val, labels_val = data_val\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs_val = model(inputs_val)\n",
        "\n",
        "                targets_val = labels_val.to(torch.float32)\n",
        "                outputs_val = outputs_val.to(torch.float32)\n",
        "\n",
        "                total_f1_score_val += f1_score(targets_val.detach().numpy(), torch.round(outputs_val).detach().numpy())\n",
        "                count_val += 1\n",
        "        \n",
        "            training_score.append(f1_score_train)\n",
        "            validation_score.append(total_f1_score_val/count_val)\n",
        "            loss_score.append(running_loss/targets.shape[0])\n",
        "\n",
        "            #print(f'training_score: {f1_score_train} validation_score: {total_f1_score_val/count_val} loss_score {loss_score}')\n",
        "\n",
        "            #set the metrics to 0\n",
        "            running_loss = 0\n",
        "            total_acc, total_count = 0, 0\n",
        "\n",
        "    results_vanilla['accuracy_validation'].append(validation_score)\n",
        "    results_vanilla['accuracy_training'].append(training_score)\n",
        "    results_vanilla['loss'].append(loss_score)"
      ],
      "metadata": {
        "id": "vKIMHNSVbUQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_vanilla['accuracy_validation'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbQagqrTxkm3",
        "outputId": "e11b105b-8a6d-475d-9b2b-10b264be5fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7140273831051407, 0.7125097125097125, 0.70958930836016, 0.7133111319473633, 0.7156695523444792, 0.7084418968212611, 0.7106493506493506, 0.7116226292754093, 0.70958930836016, 0.7106493506493506, 0.7133111319473633, 0.7133111319473633, 0.70958930836016, 0.7116226292754093, 0.7156695523444792, 0.7133111319473633, 0.7156695523444792, 0.7152061855670102, 0.7133111319473633, 0.7152061855670102, 0.7106493506493506, 0.7133111319473633, 0.7125097125097125, 0.70958930836016, 0.7133111319473633, 0.70958930836016, 0.7116226292754093, 0.7140273831051407, 0.7118682901283058, 0.7268466632705043, 0.7265291607396871, 0.7481330340746786, 0.75446080455025, 0.7534742611078489, 0.759335686937497, 0.7528395757063737, 0.7472658920027342, 0.7465531224655313, 0.7386724960254373, 0.7400379506641367, 0.7489311345243548, 0.7306983260821474, 0.7340712045748017, 0.7342342342342343, 0.7383525370357118, 0.7374931581828135, 0.7415362731152204, 0.7453007518796992, 0.7455536214334367, 0.7450135434622014, 0.7564411492122336, 0.775, 0.7696127110228401, 0.7734591194968554, 0.7582768248903071, 0.7171023151605675, 0.6925566343042071, 0.6969624300559553, 0.6947819314641746, 0.7109724770642201, 0.71438263229308, 0.7193986065273195, 0.7085714285714286, 0.724695283518813, 0.7646697388632873, 0.7709923664122138, 0.7816601049868765, 0.8008897070102776, 0.8093917274939173, 0.8079282680509674, 0.8081801470588236, 0.7843722782132636, 0.7686019954321432, 0.7713882163034707, 0.7828399145764415, 0.8035664659564614, 0.7953037007240547, 0.8067026354015354, 0.7968535469107552, 0.7775258690849901, 0.7475012493753124, 0.7562949640287769, 0.7266787658802178, 0.7047216349541932, 0.720152976388427, 0.7123885918003565, 0.7166214995483289, 0.7207275165535161, 0.726984126984127, 0.7510487586823464, 0.7572782895179183, 0.7645801910507792, 0.7741874527588813, 0.7785087719298246, 0.7945578231292517, 0.7890503991421423, 0.7934426229508197, 0.7911332385016596, 0.7879020813623463, 0.7947118241235888, 0.790685288199787, 0.7797389225960655, 0.7873501199040768, 0.789787798408488, 0.7862110311750599, 0.791263873970641, 0.7982300884955753, 0.7922108208955223, 0.7904109589041097, 0.7909090909090909, 0.7632275132275133, 0.7486895748398368, 0.7517361111111112, 0.7647804054054054, 0.7854277635299534, 0.7916096671226631, 0.7914248268114585, 0.7873873873873873, 0.7891030392883618, 0.792004048582996, 0.79004004004004, 0.7849047504219917, 0.7925531914893618, 0.8008702622237489, 0.7937419430446502, 0.8085714285714285, 0.7998290598290598, 0.802156690140845, 0.7928419601309817, 0.8045729076790338, 0.8076923076923077, 0.7997089932284963, 0.8164777680906713, 0.8083476272155518, 0.8019003743161532, 0.8010828625235404, 0.8097662578106919, 0.7997577016268604, 0.8066666666666668, 0.8112903225806452, 0.8083164300202841, 0.8014001262408906, 0.8019250253292807, 0.8061933356882138, 0.7805885389778007, 0.7834777269936558, 0.7765837104072397, 0.7612781954887218, 0.7486854314877822, 0.7396031061259706, 0.7303636363636363, 0.7225070721357849, 0.738974358974359, 0.7456896551724138, 0.7490500444660038, 0.7621155822594672, 0.8003067912425045, 0.8154064388799802, 0.8180034338974737, 0.8192956496004735, 0.8185866635415446, 0.8249586776859504, 0.8258304195804196, 0.8295429575917381, 0.8300280112044818, 0.8340170940170939, 0.8471250851024323, 0.8352287581699347, 0.8261831048208756, 0.8169160702667535, 0.8118142895370618, 0.8232501443510619, 0.8343689198518797, 0.8532289628180039, 0.8402131308182264, 0.8483432455395071, 0.8489844683393071, 0.8398148148148148, 0.8266212894687087, 0.8396551724137931, 0.8335238351374974, 0.8328187919463088, 0.8335831107056754, 0.8207885304659499, 0.852389594676346, 0.844538766270515, 0.8329347999362346, 0.8250909090909091, 0.8149143767423337, 0.8094823836450631, 0.8118191721132899, 0.8085555256306671, 0.8037800055182562, 0.8169367283950617, 0.8224246166512298, 0.8408515788315547, 0.8484576757532281, 0.831033831033831, 0.8153209109730848, 0.8055555555555556, 0.8108527131782945, 0.8057692307692308, 0.7977676149135725, 0.7991030003093103, 0.8, 0.7908925606767333, 0.7977676149135726, 0.8064736203759855, 0.8248081841432225, 0.8306871421134825, 0.8398338750451426, 0.8415370327374041, 0.837967401725791, 0.8365139949109415, 0.8250487329434697, 0.8171779141104294, 0.8181818181818181, 0.8245720345402211, 0.8307692307692307, 0.8468720821661999, 0.8469191633691351, 0.8467948717948717, 0.8226072607260726, 0.8062937062937063, 0.7946639150943395, 0.7950125829329673, 0.7870967741935484, 0.7773109243697479, 0.7698689231790277, 0.7628729647078271, 0.7621671258034894, 0.7689269256089533, 0.7670960559796438, 0.7876916037520019, 0.7979068396226414, 0.8048726467331118, 0.797549678768863, 0.8105279359704479, 0.7982009879820099, 0.8029994175888177, 0.8145345516847515, 0.8304195804195804, 0.8382023950519806, 0.8485917751972798, 0.8444018072662004, 0.8455081669691469, 0.8435862068965518, 0.8292709232096634, 0.8361746361746363, 0.8277456647398844, 0.8400084763721127, 0.8383928571428572, 0.8428547495392863, 0.8604582843713279, 0.8514755995627853, 0.8496986194827921, 0.8453930244664237, 0.845230369889682, 0.8448051948051949, 0.8372994652406418, 0.8385319837499139, 0.8318626120010861, 0.8348901098901099, 0.8291955617198336, 0.8306326901095946, 0.845890844624541, 0.8568029286150092, 0.8593142453163662, 0.8565068493150685, 0.8384687208216621, 0.8310983397190294, 0.8329547800949825, 0.8189704754601227, 0.808284023668639, 0.8113847988624374, 0.8028735632183908, 0.8300241886716389, 0.83335144533797, 0.8520448179271709, 0.8477922699533326, 0.844108721467212, 0.823825384451846, 0.8127897564206343, 0.7857484457197512, 0.7654811185266537]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we find the average at each timestep for the model's accuracy on the validation data"
      ],
      "metadata": {
        "id": "TzduuLW3CcTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_avg_vanilla_bilstm = {\n",
        "    'accuracy_training': [],\n",
        "    'loss': [],\n",
        "    'accuracy_validation': []\n",
        "}\n",
        "\n",
        "#get the averages during the training phases\n",
        "for k,v in results_vanilla.items():\n",
        "    score_raw = results_vanilla[k]\n",
        "    score_avg = []\n",
        "    for r in range(0, len(score_raw[0])):\n",
        "        avg = 0\n",
        "        for i in range(0, len(score_raw)):\n",
        "            avg += score_raw[i][r]\n",
        "        score_avg.append(avg/len(score_raw))\n",
        "    results_avg_vanilla_bilstm[k] = score_avg"
      ],
      "metadata": {
        "id": "rhkVWc5dTuDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Bi-LSTM model, we reach a score of around 82% on the test set!"
      ],
      "metadata": {
        "id": "QnA7NvT2jOX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "f1 = 0\n",
        "total_count = 0\n",
        "for _, d in enumerate(val_loader_sexist):\n",
        "    inputs, labels = d\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    targets = labels.to(torch.float32)\n",
        "    outputs = outputs.to(torch.float32)\n",
        "    \n",
        "    f1 = f1_score(targets.detach().numpy(), torch.round(outputs).detach().numpy())\n",
        "\n",
        "    f1 += f1\n",
        "    total_count += 1\n",
        "\n",
        "f1 = f1/total_count\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFkk1qXSvBin",
        "outputId": "d527b290-8234-4575-9400-a8fb8230b412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8431372549019608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Creating the MAML algorithm using a class\n",
        "\n",
        "\n",
        "*   MAML.evaluate() will train the learner model on batches of 18 points 4 times, saving the learner's accuracy with each new batch it sees\n",
        "*   MAML loop will run the MAML algorithm for 1 only (so 1 iteration of the outer loop and n_updates for each task in the inner loop)\n",
        "\n"
      ],
      "metadata": {
        "id": "iWeFvw4BskBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding layers\n",
        "\n",
        "#Sexist embedding layers\n",
        "embedding_sexist = nn.Embedding(dataset_sexist.embedding_matrix.shape[0], 50)\n",
        "embedding_sexist.weight=nn.Parameter(torch.tensor(dataset_sexist.embedding_matrix, dtype=torch.float32))\n",
        "\n",
        "#Amazon embedding layers\n",
        "embedding_amazon = nn.Embedding(dataset_amazon.embedding_matrix.shape[0], 50)\n",
        "embedding_amazon.weight=nn.Parameter(torch.tensor(dataset_amazon.embedding_matrix, dtype=torch.float32))\n",
        "\n",
        "#IMDB embedding layers\n",
        "embedding_imdb = nn.Embedding(dataset_imdb.embedding_matrix.shape[0], 50)\n",
        "embedding_imdb.weight=nn.Parameter(torch.tensor(dataset_imdb.embedding_matrix, dtype=torch.float32))"
      ],
      "metadata": {
        "id": "d_J1QhE9T5O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.adam import Adam\n",
        "import copy\n",
        "\n",
        "class MAML():\n",
        "    def __init__(self, epochs, inner_updates, loss_fn, inner_stepsize, meta_stepsize, dict_embeddings, args_learner, tasks_list, target_task) -> None:\n",
        "        self.epochs = epochs\n",
        "        self.inner_updates = inner_updates \n",
        "        self.learner = BiLSTM(args_learner)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.inner_stepsize = inner_stepsize\n",
        "        self.meta_step_size = meta_stepsize\n",
        "        self.embedding_dict = dict_embeddings\n",
        "        self.tasks_list = tasks_list #list of (train_loader, val_loader, key)\n",
        "        self.target_train_loader, self.target_val_loader, self.key = target_task #thruple (train_loader, val_loader, key)\n",
        "        self.args_learner = args_learner\n",
        "        self.results = [] #each position is the ith train call\n",
        "        self.models = []\n",
        "        self.eval = 0\n",
        "        self.t = 0\n",
        "\n",
        "    def evaluate(self, weights=None):\n",
        "        print(f'Evaluation: {self.eval}')\n",
        "        self.eval += 1\n",
        "        #Create result dict at this evaluation position\n",
        "        results = {\n",
        "            'loss' : [],\n",
        "            'accuracy_training': [],\n",
        "            'accuracy_validation' : []\n",
        "        }\n",
        "        if weights == None:\n",
        "            # Make a copy of the model\n",
        "            args = copy.deepcopy(self.args_learner)\n",
        "            args['requires_grad'] = False\n",
        "            model = BiLSTM(args)\n",
        "\n",
        "            #load the state dict cause you'll be training, don't want to mess up the weights\n",
        "            state_dict = copy.deepcopy(self.learner.state_dict())\n",
        "            model.load_state_dict(state_dict)\n",
        "        else:\n",
        "            model = BiLSTM(args_learner)\n",
        "            model.load(weights)\n",
        "\n",
        "        # Train\n",
        "        #for e in range(5):\n",
        "        for i, data in enumerate(self.target_train_loader):\n",
        "            if i == 4:\n",
        "                break\n",
        "            model.train()\n",
        "\n",
        "            total_count = 0\n",
        "            running_loss = 0\n",
        "\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optim.Adam(model.parameters(), lr=0.01).zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            #Get the loss on the batch\n",
        "            targets = labels.to(torch.float32)\n",
        "            outputs = outputs.to(torch.float32)\n",
        "            \n",
        "            loss = self.loss_fn(outputs, targets)\n",
        "\n",
        "            #gets the gradient on the batch for each parameter\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "            #take size lr step in gradient direction\n",
        "            optim.Adam(model.parameters(), lr=0.01).step()\n",
        "\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "            f1_score_train = f1_score(targets.detach().numpy(), torch.round(outputs).detach().numpy())\n",
        "\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            #validation\n",
        "            model.eval()\n",
        "\n",
        "            #set metrics to 0\n",
        "            total_f1_val = 0\n",
        "            total_count_val = 0\n",
        "\n",
        "            for i_val, data_val in enumerate(self.target_val_loader):\n",
        "                inputs_val, labels_val = data_val\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs_val = model(inputs_val)\n",
        "\n",
        "                targets_val = labels_val.to(torch.float32)\n",
        "                outputs_val = outputs_val.to(torch.float32)\n",
        "\n",
        "                total_f1_val += f1_score(targets_val.detach().numpy(), torch.round(outputs_val).detach().numpy())\n",
        "\n",
        "                total_count_val += 1\n",
        "            \n",
        "            results['accuracy_training'].append(f1_score_train)\n",
        "            results['accuracy_validation'].append(total_f1_val/total_count_val)\n",
        "            results['loss'].append(running_loss/total_count)\n",
        "\n",
        "            #print(f'acc_train: {total_acc/total_count} \\| acc_val: {total_accuracy_val/total_count_val} \\| avg_loss: {running_loss/total_count}')\n",
        "\n",
        "        self.results.append({'results':results, 't': self.t, 'i': i, 'model_state_val': copy.deepcopy(model.state_dict()), 'model_state': copy.deepcopy(self.learner.state_dict())})\n",
        "\n",
        "        print(results['accuracy_validation'], i)\n",
        "        return model.state_dict()\n",
        "\n",
        "    def maml_loop(self):\n",
        "        print(f'Start_train: {self.t}')\n",
        "        self.t += 1\n",
        "        grads = []\n",
        "        #this is the meta loop, could sample from the # of tasks\n",
        "        for (train, val, key) in self.tasks_list:\n",
        "            #create a new model for gradient update in inner loop\n",
        "            #this is just to keep track of the gradien update\n",
        "            new_model = BiLSTM(self.args_learner)\n",
        "            state_dict = copy.deepcopy(self.learner.state_dict())\n",
        "\n",
        "            #load the correct embedding\n",
        "            new_model._modules['embedding'] = self.embedding_dict[key]\n",
        "            state_dict['embedding.weight'] = self.embedding_dict[key].weight\n",
        "\n",
        "            new_model.load_state_dict(state_dict)\n",
        "            \n",
        "\n",
        "            #run the inner loop on the new model for a given task\n",
        "            #just to keep track\n",
        "            fast_weights = dict((name, param) for (name, param) in new_model.named_parameters())\n",
        "\n",
        "            for updates in range(self.inner_updates):\n",
        "                in_, target = train.__iter__().next()\n",
        "\n",
        "                out = new_model.forward(in_)\n",
        "\n",
        "                out = out.to(torch.float32)\n",
        "                target = target.to(torch.float32)\n",
        "\n",
        "                #fix issue for cross entropy, maybe fixes the other issue about auto_grad\n",
        "                loss = self.loss_fn(out, target)\n",
        "                #get the gradient for task\n",
        "                g = torch.autograd.grad(loss, new_model.parameters(), create_graph=True)\n",
        "\n",
        "                #zero the embedding gradients since we're not learning those\n",
        "                g = tuple(g[i] if i != 0 else torch.zeros(g[0].shape, requires_grad=True) for i in range(len(g)))\n",
        "\n",
        "                #update weights\n",
        "                fast_weights = dict((name, param - self.inner_stepsize * g) for ((name, param), g) in zip(fast_weights.items(), g))\n",
        "\n",
        "                #load those weights into the new_model, embedding weights is the same\n",
        "                new_model.load_state_dict(fast_weights)\n",
        "\n",
        "            #Test the net after training\n",
        "            #write some print statements for sanity checks\n",
        "\n",
        "            #compute the meta_gradient on val and return it\n",
        "            in_, target = val.__iter__().next()\n",
        "            out = new_model.forward(in_)\n",
        "            out = out.to(torch.float32)\n",
        "            target = target.to(torch.float32)\n",
        "\n",
        "            #fix issue for cross entropy, maybe fixes the other issue about auto_grad\n",
        "            loss = self.loss_fn(out, target)\n",
        "\n",
        "            #normalizes the loss over the batch size\n",
        "            loss = loss / len(tasks_list)\n",
        "\n",
        "            g = torch.autograd.grad(loss, new_model.parameters(), create_graph=True)\n",
        "\n",
        "            #zero the gradients\n",
        "            #set to the learner model embedding weights\n",
        "            learner_embedding_size = copy.deepcopy(self.learner.state_dict())\n",
        "            learner_embedding_size = learner_embedding_size['embedding.weight'].shape\n",
        "            g = tuple(g[i] if i != 0 else torch.zeros(learner_embedding_size, requires_grad=True) for i in range(len(g)))\n",
        "\n",
        "            meta_grads = {name:g for ((name, _), g) in zip(new_model.named_parameters(), g)}\n",
        "\n",
        "            #pass the meta-task gradients to the \n",
        "            grads.append(meta_grads)\n",
        "            \n",
        "        #load new task completely for dummy, this task is the target task\n",
        "        in_, target = self.target_train_loader.__iter__().next()\n",
        "\n",
        "        # Do a dummy forward/backward pass to get correct grads into learner\n",
        "        # We will be replacing the gradients with hooks regardless\n",
        "        out = self.learner.forward(in_)\n",
        "        out = out.to(torch.float32)\n",
        "        target = target.to(torch.float32)\n",
        "        loss = self.loss_fn(out, target)\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        # Unpack the gradients from dictionary of meta-gradients\n",
        "        gradients = {k: sum(d[k] for d in grads) for k in grads[0].keys()}\n",
        "        \n",
        "        hooks = []\n",
        "        for(k,v) in self.learner.named_parameters():\n",
        "            def get_closure():\n",
        "                key = k\n",
        "                def replace_grad(grad):\n",
        "                    return gradients[key]\n",
        "                return replace_grad\n",
        "            hooks.append(v.register_hook(get_closure()))\n",
        "        \n",
        "        # Compute grads for current step, replace with summed gradients as defined by hook\n",
        "        # DO DOUBLE CHECK THAT THE GRADIENTS ARE BEING UPDATED CORRECTLY WITH HOOKS\n",
        "        optim.Adam(self.learner.parameters(), lr=self.meta_step_size).zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the net parameters with the accumulated gradient according to optimizer\n",
        "        optim.Adam(self.learner.parameters(), lr=self.meta_step_size).step()\n",
        "\n",
        "        # Remove the hooks before next training phase\n",
        "        for h in hooks:\n",
        "            h.remove()\n",
        "\n",
        "        #save the state dict so it can be used later\n",
        "        self.models.append(copy.deepcopy(self.learner.state_dict()))"
      ],
      "metadata": {
        "id": "dt6mBT78yHMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the arguments for the BiLSTM model which we will use as a learner model in the MAML meta-learning algorithm"
      ],
      "metadata": {
        "id": "UqsXQjVojcC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks_list = [(train_loader_amazon, val_loader_amazon, 'amazon'), (train_loader_imdb, val_loader_imdb, 'imdb')]\n",
        "\n",
        "embedding_dict = {\n",
        "    'sexist': embedding_sexist,\n",
        "    'amazon': embedding_amazon,\n",
        "    'imdb': embedding_imdb,\n",
        "}\n",
        "\n",
        "# Here we will be changing the input matrix embedding with every different dataset so we set required_grad to True\n",
        "args_learner = {\n",
        "           'hidden_dim' : 32, #number of hidden dim\n",
        "           'vocab_size' : dataset_sexist.embedding_matrix.shape[0], #number of rows in word embedding matrix\n",
        "           'input_size' : 50,\n",
        "           'output_dim' : 1, #number of classes you're predicting\n",
        "           'embedding_matrix' : dataset_sexist.embedding_matrix, #the embedding matrix you've built in dataset constructor\n",
        "           'drp' : 0.2, #dropout layer for forward LSTM\n",
        "           'requires_grad': True,\n",
        "    }"
      ],
      "metadata": {
        "id": "63ik73HFehxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we get the average of the results of the learner model train with MAML. We use this to determine how many iterations are required to get a good meta-trained model.\n",
        "\n",
        "One of the big issues with the MAML algorithm is instability in the learner model's ability to learn meta-parameters as well as computational power needed to find optimal parameters. Below we tested different numbers of outer loop numbers. Before I did some tests to figure out the best learning rates. They were inconclusive however. Generally, this is one of the biggest downsides of the experiment: we did not find MAML hyperparameters which very consistently produced a model which had \"laerned how to learn\". However, we did find that given the learning rates and number of inner loop updates, meta-parameters were generlly learned after about 30 meta-updates.\n",
        "\n",
        "We use as criteria to determine whether a model has learned meta-learning parameters well how well the model performs after being fed four batches of 18 datapoints and evaluating it with each batch."
      ],
      "metadata": {
        "id": "k5tYdUlujlSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "train_loader_sexist_maml = DataLoader(trainset_sexist, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader_sexist_maml = DataLoader(valset_sexist, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# Mother of all experiments\n",
        "# Get 10 values for this step size\n",
        "# This gave us the below strings\n",
        "# This also takes about 4 hours\n",
        "\n",
        "# Set to 10\n",
        "for lr in range(10):\n",
        "    maml = MAML(epochs=15, \n",
        "    inner_updates=2, \n",
        "    loss_fn=nn.BCELoss(), \n",
        "    inner_stepsize=0.01, \n",
        "    meta_stepsize=0.01, \n",
        "    dict_embeddings=embedding_dict, \n",
        "    args_learner=args_learner, \n",
        "    tasks_list = tasks_list, \n",
        "    target_task = (train_loader_sexist_maml, val_loader_sexist_maml, 'sexist'))\n",
        "\n",
        "    #Set to 50\n",
        "    for i in range(10):\n",
        "        maml.maml_loop()\n",
        "        maml.evaluate()\n",
        "    results.append(copy.deepcopy(maml.results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6F2SBi2DZCO",
        "outputId": "ca7f7839-2a49-4fe7-9122-df9ea79e2ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.0, 0.7058823529411764, 0.045183714001986106, 0.0] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.0, 0.0, 0.0, 0.0] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.0, 0.0, 0.0, 0.7044688165002455] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.0, 0.0, 0.7075667893137098, 0.0] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.4171374764595104, 0.6007625272331154, 0.7106493506493506, 0.7152061855670102] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.7058823529411764, 0.7116226292754093, 0.7116226292754093, 0.38383838383838387] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.7133111319473633, 0.7058823529411764, 0.70958930836016, 0.7044688165002455] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.0, 0.0, 0.7133111319473633, 0.7146589259796807] 4\n",
            "Start_train: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f456ed1dcb0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f456ed1dcb0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f456ed1dcb0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f456ed1dcb0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04223521767381416, 0.0, 0.0, 0.6598186728395061] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.5528765815230365, 0.7125097125097125, 0.7166202718192238, 0.7143272304562627] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.70958930836016, 0.7133111319473633, 0.7058823529411764, 0.7125097125097125] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.6428571428571428, 0.7116226292754093, 0.7116226292754093, 0.7116226292754093] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.30828282828282827, 0.0, 0.6985294117647058, 0.7146589259796807] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.7116226292754093, 0.7072064724496787, 0.7156695523444792, 0.7140273831051407] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.7106493506493506, 0.6770297390987046, 0.70958930836016, 0.70958930836016] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.4080578512396694, 0.7021671416263503, 0.7106493506493506, 0.6552970209765556] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.70958930836016, 0.6967274324621129, 0.7044688165002455, 0.6678863745787194] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.0, 0.22673594709494566, 0.7008520998174073, 0.7027223230490018] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.04477611940298507, 0.6676915322580645, 0.6299452221545951, 0.3827501089008276] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.680405142189326, 0.051948051948051945, 0.6344792462439521, 0.7072064724496787] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.6827284105131415, 0.0, 0.0, 0.5373357228195939] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.6646030026311717, 0.0, 0.5863695510735198, 0.7133111319473633] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.0722943722943723, 0.7074332702473405, 0.7160493827160493, 0.708374063212773] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.7057786218122626, 0.7163459994218354, 0.7106493506493506, 0.7116226292754093] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.5533199195171026, 0.0, 0.6487603305785123, 0.020833333333333336] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.0, 0.0, 0.3978481012658228, 0.0] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.7064052287581699, 0.613740383679034, 0.708374063212773, 0.7125097125097125] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.017857142857142856, 0.4376337008392299, 0.7077922077922078, 0.4915396213771151] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.7115441296872442, 0.6164229471316085, 0.3969979296066253, 0.7101686589159981] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.0, 0.07936507936507936, 0.7059649122807017, 0.6665526675786594] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.6052105263157894, 0.7133111319473633, 0.7146589259796807, 0.70958930836016] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.0, 0.0, 0.0, 0.0] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.7153846153846154, 0.0, 0.6910992944774614, 0.7125097125097125] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.7106493506493506, 0.7146589259796807, 0.6939519768911357, 0.7116226292754093] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.45464535464535455, 0.7141025641025641, 0.25024098708309234, 0.7066666666666667] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.2401719901719902, 0.7116226292754093, 0.7166907166907167, 0.7125097125097125] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.7020202020202021, 0.24795277090078077, 0.0, 0.440271725198667] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.7054681301243058, 0.5293091884641181, 0.0, 0.014492753623188406] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.3021621621621622, 0.7106493506493506, 0.7106493506493506, 0.6966478948779834] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.7072064724496787, 0.7147308587864676, 0.047982812126999286, 0.16473093302361597] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.0, 0.7156695523444792, 0.0, 0.713926776740847] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.7160493827160493, 0.7156695523444792, 0.7119593881856541, 0.7106493506493506] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.7013704032337572, 0.7140273831051407, 0.0, 0.7116226292754093] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.7125097125097125, 0.7160493827160493, 0.7044688165002455, 0.7090903018968742] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.3647774327122153, 0.7122153209109731, 0.602166160348699, 0.6854203935599286] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.7116226292754093, 0.7133111319473633, 0.7146589259796807, 0.4997198879551821] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.4158562367864693, 0.7099961003509685, 0.5979023456877612, 0.7133111319473633] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.7160493827160493, 0.70958930836016, 0.7106493506493506, 0.7133111319473633] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.5998317914213624, 0.699951710816777, 0.7108826727290698, 0.4766081871345029] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.7092281660809581, 0.7164300071322052, 0.6201286650995917, 0.6180048661800487] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.7125097125097125, 0.04818401937046004, 0.7417672992141078, 0.7521568627450981] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.0, 0.0, 0.0, 0.70958930836016] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.7217140105922003, 0.754043804659082, 0.7116226292754093, 0.7301231802911533] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.0, 0.7116226292754093, 0.7029651010233535, 0.5068027210884354] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.0, 0.7194444444444446, 0.7182194414572947, 0.5073593073593073] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.7133111319473633, 0.70958930836016, 0.7215517531179615, 0.6312056737588653] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.6844018506278915, 0.7146589259796807, 0.7116226292754093, 0.7152631578947368] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.2365728900255754, 0.7184234099731261, 0.7235659529473961, 0.70958930836016] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.6433178794597864, 0.6981216095862683, 0.7133111319473633, 0.70958930836016] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.19383490073145246, 0.7088669950738916, 0.5233389402859546, 0.697699983088111] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.7152061855670102, 0.7125097125097125, 0.05799220272904483, 0.7441659464131374] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.7152417474998121, 0.0, 0.7226240707253365, 0.7125097125097125] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.7106493506493506, 0.7208261617900171, 0.7188521241830066, 0.5589414595028066] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.7258014316837845, 0.7106493506493506, 0.6215541165587419, 0.713057729614021] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.7140273831051407, 0.7058823529411764, 0.7084418968212611, 0.7121588089330024] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.6041811846689895, 0.7167056261807936, 0.7189674597388827, 0.7133111319473633] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.0, 0.4125, 0.7084418968212611, 0.60858189429618] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.5849268841394826, 0.7169271964104791, 0.7116226292754093, 0.720661672908864] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.717223991507431, 0.1819320214669052, 0.03797468354430379, 0.6518550474547025] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.646078431372549, 0.7193636589609744, 0.7137745301141286, 0.35354838709677416] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.0, 0.7163459994218354, 0.7140273831051407, 0.7058823529411764] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.7125097125097125, 0.29991356957649096, 0.01886792452830189, 0.6281827694454133] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.5122310639552018, 0.0, 0.6516129032258065, 0.7140273831051407] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.7140273831051407, 0.7165596919127086, 0.7125097125097125, 0.7133111319473633] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.0, 0.6647751155947879, 0.4722222222222222, 0.70958930836016] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.0, 0.4577922077922078, 0.01388888888888889, 0.6986795702452226] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.7106493506493506, 0.71, 0.7146297411198073, 0.7146589259796807] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.6296780757212412, 0.020408163265306124, 0.06987295825771324, 0.7258863896301944] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.11729957805907174, 0.7225877192982455, 0.713926776740847, 0.41742886178861793] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.03333333333333333, 0.6755218216318786, 0.03333333333333333, 0.0753968253968254] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.365260129465356, 0.7167392976159959, 0.7116226292754093, 0.7146589259796807] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.7106493506493506, 0.638888888888889, 0.13574858140549978, 0.0] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.7140273831051407, 0.7106493506493506, 0.07502329916123018, 0.7084418968212611] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.7140273831051407, 0.3682950191570881, 0.5427884615384615, 0.7140273831051407] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.6941535621738668, 0.0, 0.7116226292754093, 0.10318161732213874] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.0, 0.4904761904761904, 0.7084418968212611, 0.70958930836016] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.1266025641025641, 0.70958930836016, 0.5322105263157895, 0.7116226292754093] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.0, 0.6813618108798831, 0.7084418968212611, 0.705996062287453] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.5641800712512144, 0.3686746987951808, 0.7174825174825175, 0.6823583602026715] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.6513343937787204, 0.7074332702473406, 0.70958930836016, 0.7106493506493506] 4\n",
            "Start_train: 0\n",
            "Evaluation: 0\n",
            "[0.7029651010233535, 0.7133111319473633, 0.7058823529411764, 0.7129411764705882] 4\n",
            "Start_train: 1\n",
            "Evaluation: 1\n",
            "[0.0, 0.7152061855670102, 0.1392857142857143, 0.7175646761330481] 4\n",
            "Start_train: 2\n",
            "Evaluation: 2\n",
            "[0.4770302043769217, 0.7133111319473633, 0.3310931899641577, 0.0] 4\n",
            "Start_train: 3\n",
            "Evaluation: 3\n",
            "[0.70958930836016, 0.16873065015479877, 0.7167057127410346, 0.7058823529411764] 4\n",
            "Start_train: 4\n",
            "Evaluation: 4\n",
            "[0.4043753038405445, 0.7133111319473633, 0.5658263305322129, 0.5300099700897307] 4\n",
            "Start_train: 5\n",
            "Evaluation: 5\n",
            "[0.0, 0.07167308515190415, 0.08087201125175808, 0.6933333333333334] 4\n",
            "Start_train: 6\n",
            "Evaluation: 6\n",
            "[0.7027914614121511, 0.6168394253500636, 0.7072064724496787, 0.6829220524872698] 4\n",
            "Start_train: 7\n",
            "Evaluation: 7\n",
            "[0.39914304548450896, 0.7160493827160493, 0.7007365249414128, 0.7133111319473633] 4\n",
            "Start_train: 8\n",
            "Evaluation: 8\n",
            "[0.7074332702473406, 0.6812729498164014, 0.7116226292754093, 0.7174236542443064] 4\n",
            "Start_train: 9\n",
            "Evaluation: 9\n",
            "[0.5501944404436123, 0.0, 0.28955223880597014, 0.6964411806242057] 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we save the best performing model after around 30 meta-updates"
      ],
      "metadata": {
        "id": "0p9OZ6giFLPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "\n",
        "#save with optimal params\n",
        "optimal_val = results[x]['model_state_val']\n",
        "optimal_state = results[x]['model_state']\n",
        "torch.save(optimal_val, \"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state_val\")\n",
        "torch.save(optimal_state, \"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state\")\n",
        "\n",
        "#save with best, it was model after 36 iterations\n",
        "best_val = results[x]['model_state_val']\n",
        "best_state = results[x]['model_state']\n",
        "torch.save(best_val, \"/content/drive/MyDrive/COMP550 Final Project/Models/Best/model_state_val\")\n",
        "torch.save(best_state, \"/content/drive/MyDrive/COMP550 Final Project/Models/Best/model_state\")"
      ],
      "metadata": {
        "id": "3z_5GYBm4RTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below I process the results of the experiment to figure out how many meta-updates generally returns a good model with easily generalizeable parameters.\n"
      ],
      "metadata": {
        "id": "pluv-X1O2nxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_strings(s):\n",
        "    for i in range(50):\n",
        "        s = s.replace(f'Start_train: {i} Evaluation: {i}', '')\n",
        "    s = s.split(' 4')\n",
        "    s = [i.replace(' ', '') for i in s]\n",
        "    s.pop()\n",
        "    s = [float(i) for i in s]\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "ilrkLecqhp6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the results of the loop which found the best number of loops to train the meta-learner."
      ],
      "metadata": {
        "id": "OCMl6EhIkarU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = \"Start_train: 0 Evaluation: 0 0.6096491228070176 4 Start_train: 1 Evaluation: 1 0.7017543859649122 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5701754385964912 4 Start_train: 5 Evaluation: 5 0.5921052631578947 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5789473684210527 4 Start_train: 8 Evaluation: 8 0.5570175438596491 4 Start_train: 9 Evaluation: 9 0.5570175438596491 4 Start_train: 10 Evaluation: 10 0.5877192982456141 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.5570175438596491 4 Start_train: 13 Evaluation: 13 0.618421052631579 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.6929824561403509 4 Start_train: 16 Evaluation: 16 0.618421052631579 4 Start_train: 17 Evaluation: 17 0.6973684210526315 4 Start_train: 18 Evaluation: 18 0.5570175438596491 4 Start_train: 19 Evaluation: 19 0.5745614035087719 4 Start_train: 20 Evaluation: 20 0.706140350877193 4 Start_train: 21 Evaluation: 21 0.6973684210526315 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.7149122807017544 4 Start_train: 24 Evaluation: 24 0.6491228070175439 4 Start_train: 25 Evaluation: 25 0.5657894736842105 4 Start_train: 26 Evaluation: 26 0.5657894736842105 4 Start_train: 27 Evaluation: 27 0.6140350877192983 4 Start_train: 28 Evaluation: 28 0.6929824561403509 4 Start_train: 29 Evaluation: 29 0.6798245614035088 4 Start_train: 30 Evaluation: 30 0.6842105263157895 4 Start_train: 31 Evaluation: 31 0.5570175438596491 4 Start_train: 32 Evaluation: 32 0.5789473684210527 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5657894736842105 4 Start_train: 35 Evaluation: 35 0.5833333333333334 4 Start_train: 36 Evaluation: 36 0.6666666666666666 4 Start_train: 37 Evaluation: 37 0.631578947368421 4 Start_train: 38 Evaluation: 38 0.6842105263157895 4 Start_train: 39 Evaluation: 39 0.6228070175438597 4 Start_train: 40 Evaluation: 40 0.6052631578947368 4 Start_train: 41 Evaluation: 41 0.6403508771929824 4 Start_train: 42 Evaluation: 42 0.5657894736842105 4 Start_train: 43 Evaluation: 43 0.6578947368421053 4 Start_train: 44 Evaluation: 44 0.5701754385964912 4 Start_train: 45 Evaluation: 45 0.5833333333333334 4 Start_train: 46 Evaluation: 46 0.5570175438596491 4 Start_train: 47 Evaluation: 47 0.5570175438596491 4 Start_train: 48 Evaluation: 48 0.5745614035087719 4 Start_train: 49 Evaluation: 49 0.7017543859649122 4\"\n",
        "c2 = \"Start_train: 0 Evaluation: 0 0.5614035087719298 4 Start_train: 1 Evaluation: 1 0.6403508771929824 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.6885964912280702 4 Start_train: 7 Evaluation: 7 0.5570175438596491 4 Start_train: 8 Evaluation: 8 0.6271929824561403 4 Start_train: 9 Evaluation: 9 0.631578947368421 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.7149122807017544 4 Start_train: 12 Evaluation: 12 0.618421052631579 4 Start_train: 13 Evaluation: 13 0.44298245614035087 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.5701754385964912 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.6535087719298246 4 Start_train: 18 Evaluation: 18 0.6535087719298246 4 Start_train: 19 Evaluation: 19 0.6447368421052632 4 Start_train: 20 Evaluation: 20 0.5570175438596491 4 Start_train: 21 Evaluation: 21 0.5921052631578947 4 Start_train: 22 Evaluation: 22 0.6842105263157895 4 Start_train: 23 Evaluation: 23 0.7236842105263158 4 Start_train: 24 Evaluation: 24 0.6447368421052632 4 Start_train: 25 Evaluation: 25 0.5921052631578947 4 Start_train: 26 Evaluation: 26 0.6622807017543859 4 Start_train: 27 Evaluation: 27 0.5570175438596491 4 Start_train: 28 Evaluation: 28 0.5570175438596491 4 Start_train: 29 Evaluation: 29 0.6447368421052632 4 Start_train: 30 Evaluation: 30 0.5614035087719298 4 Start_train: 31 Evaluation: 31 0.6622807017543859 4 Start_train: 32 Evaluation: 32 0.6403508771929824 4 Start_train: 33 Evaluation: 33 0.6008771929824561 4 Start_train: 34 Evaluation: 34 0.6754385964912281 4 Start_train: 35 Evaluation: 35 0.7236842105263158 4 Start_train: 36 Evaluation: 36 0.5657894736842105 4 Start_train: 37 Evaluation: 37 0.5701754385964912 4 Start_train: 38 Evaluation: 38 0.5570175438596491 4 Start_train: 39 Evaluation: 39 0.5657894736842105 4 Start_train: 40 Evaluation: 40 0.6798245614035088 4 Start_train: 41 Evaluation: 41 0.5657894736842105 4 Start_train: 42 Evaluation: 42 0.5701754385964912 4 Start_train: 43 Evaluation: 43 0.5614035087719298 4 Start_train: 44 Evaluation: 44 0.7105263157894737 4 Start_train: 45 Evaluation: 45 0.5833333333333334 4 Start_train: 46 Evaluation: 46 0.5833333333333334 4 Start_train: 47 Evaluation: 47 0.5657894736842105 4 Start_train: 48 Evaluation: 48 0.6842105263157895 4 Start_train: 49 Evaluation: 49 0.5964912280701754 4\"\n",
        "c3 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5745614035087719 4 Start_train: 8 Evaluation: 8 0.5570175438596491 4 Start_train: 9 Evaluation: 9 0.6929824561403509 4 Start_train: 10 Evaluation: 10 0.6973684210526315 4 Start_train: 11 Evaluation: 11 0.5745614035087719 4 Start_train: 12 Evaluation: 12 0.6228070175438597 4 Start_train: 13 Evaluation: 13 0.6754385964912281 4 Start_train: 14 Evaluation: 14 0.618421052631579 4 Start_train: 15 Evaluation: 15 0.5614035087719298 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.6535087719298246 4 Start_train: 18 Evaluation: 18 0.7105263157894737 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.706140350877193 4 Start_train: 21 Evaluation: 21 0.7017543859649122 4 Start_train: 22 Evaluation: 22 0.7412280701754386 4 Start_train: 23 Evaluation: 23 0.5570175438596491 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.6578947368421053 4 Start_train: 26 Evaluation: 26 0.5570175438596491 4 Start_train: 27 Evaluation: 27 0.5570175438596491 4 Start_train: 28 Evaluation: 28 0.6842105263157895 4 Start_train: 29 Evaluation: 29 0.6491228070175439 4 Start_train: 30 Evaluation: 30 0.5570175438596491 4 Start_train: 31 Evaluation: 31 0.5570175438596491 4 Start_train: 32 Evaluation: 32 0.5833333333333334 4 Start_train: 33 Evaluation: 33 0.5789473684210527 4 Start_train: 34 Evaluation: 34 0.5570175438596491 4 Start_train: 35 Evaluation: 35 0.6666666666666666 4 Start_train: 36 Evaluation: 36 0.5570175438596491 4 Start_train: 37 Evaluation: 37 0.5570175438596491 4 Start_train: 38 Evaluation: 38 0.618421052631579 4 Start_train: 39 Evaluation: 39 0.5570175438596491 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.6052631578947368 4 Start_train: 42 Evaluation: 42 0.5701754385964912 4 Start_train: 43 Evaluation: 43 0.6929824561403509 4 Start_train: 44 Evaluation: 44 0.6622807017543859 4 Start_train: 45 Evaluation: 45 0.6008771929824561 4 Start_train: 46 Evaluation: 46 0.7149122807017544 4 Start_train: 47 Evaluation: 47 0.6096491228070176 4 Start_train: 48 Evaluation: 48 0.5921052631578947 4 Start_train: 49 Evaluation: 49 0.631578947368421 4\"\n",
        "c4 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.6622807017543859 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.6535087719298246 4 Start_train: 8 Evaluation: 8 0.6535087719298246 4 Start_train: 9 Evaluation: 9 0.5570175438596491 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6008771929824561 4 Start_train: 13 Evaluation: 13 0.5570175438596491 4 Start_train: 14 Evaluation: 14 0.6798245614035088 4 Start_train: 15 Evaluation: 15 0.5570175438596491 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.5570175438596491 4 Start_train: 18 Evaluation: 18 0.5570175438596491 4 Start_train: 19 Evaluation: 19 0.6885964912280702 4 Start_train: 20 Evaluation: 20 0.5570175438596491 4 Start_train: 21 Evaluation: 21 0.5570175438596491 4 Start_train: 22 Evaluation: 22 0.6052631578947368 4 Start_train: 23 Evaluation: 23 0.6578947368421053 4 Start_train: 24 Evaluation: 24 0.5701754385964912 4 Start_train: 25 Evaluation: 25 0.5570175438596491 4 Start_train: 26 Evaluation: 26 0.5570175438596491 4 Start_train: 27 Evaluation: 27 0.6140350877192983 4 Start_train: 28 Evaluation: 28 0.6885964912280702 4 Start_train: 29 Evaluation: 29 0.6578947368421053 4 Start_train: 30 Evaluation: 30 0.5570175438596491 4 Start_train: 31 Evaluation: 31 0.6008771929824561 4 Start_train: 32 Evaluation: 32 0.543859649122807 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.6710526315789473 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.5570175438596491 4 Start_train: 37 Evaluation: 37 0.6228070175438597 4 Start_train: 38 Evaluation: 38 0.5789473684210527 4 Start_train: 39 Evaluation: 39 0.7149122807017544 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.5833333333333334 4 Start_train: 42 Evaluation: 42 0.6096491228070176 4 Start_train: 43 Evaluation: 43 0.6535087719298246 4 Start_train: 44 Evaluation: 44 0.6754385964912281 4 Start_train: 45 Evaluation: 45 0.6754385964912281 4 Start_train: 46 Evaluation: 46 0.5921052631578947 4 Start_train: 47 Evaluation: 47 0.706140350877193 4 Start_train: 48 Evaluation: 48 0.5657894736842105 4 Start_train: 49 Evaluation: 49 0.6973684210526315 4\"\n",
        "c5 = \"Start_train: 0 Evaluation: 0 0.706140350877193 4 Start_train: 1 Evaluation: 1 0.6491228070175439 4 Start_train: 2 Evaluation: 2 0.5745614035087719 4 Start_train: 3 Evaluation: 3 0.6710526315789473 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5877192982456141 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5614035087719298 4 Start_train: 8 Evaluation: 8 0.5570175438596491 4 Start_train: 9 Evaluation: 9 0.6140350877192983 4 Start_train: 10 Evaluation: 10 0.5789473684210527 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.5570175438596491 4 Start_train: 13 Evaluation: 13 0.5921052631578947 4 Start_train: 14 Evaluation: 14 0.5833333333333334 4 Start_train: 15 Evaluation: 15 0.7192982456140351 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.6798245614035088 4 Start_train: 18 Evaluation: 18 0.618421052631579 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.5745614035087719 4 Start_train: 21 Evaluation: 21 0.6578947368421053 4 Start_train: 22 Evaluation: 22 0.6271929824561403 4 Start_train: 23 Evaluation: 23 0.6535087719298246 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.6578947368421053 4 Start_train: 26 Evaluation: 26 0.7149122807017544 4 Start_train: 27 Evaluation: 27 0.6535087719298246 4 Start_train: 28 Evaluation: 28 0.5657894736842105 4 Start_train: 29 Evaluation: 29 0.6052631578947368 4 Start_train: 30 Evaluation: 30 0.7236842105263158 4 Start_train: 31 Evaluation: 31 0.6228070175438597 4 Start_train: 32 Evaluation: 32 0.5657894736842105 4 Start_train: 33 Evaluation: 33 0.6403508771929824 4 Start_train: 34 Evaluation: 34 0.5701754385964912 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.6403508771929824 4 Start_train: 37 Evaluation: 37 0.6710526315789473 4 Start_train: 38 Evaluation: 38 0.5877192982456141 4 Start_train: 39 Evaluation: 39 0.6008771929824561 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.6973684210526315 4 Start_train: 42 Evaluation: 42 0.5614035087719298 4 Start_train: 43 Evaluation: 43 0.706140350877193 4 Start_train: 44 Evaluation: 44 0.6447368421052632 4 Start_train: 45 Evaluation: 45 0.6754385964912281 4 Start_train: 46 Evaluation: 46 0.5745614035087719 4 Start_train: 47 Evaluation: 47 0.6622807017543859 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.5877192982456141 4\"\n",
        "c6 = \"Start_train: 0 Evaluation: 0 0.6798245614035088 4 Start_train: 1 Evaluation: 1 0.7105263157894737 4 Start_train: 2 Evaluation: 2 0.5921052631578947 4 Start_train: 3 Evaluation: 3 0.7412280701754386 4 Start_train: 4 Evaluation: 4 0.6359649122807017 4 Start_train: 5 Evaluation: 5 0.6491228070175439 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5570175438596491 4 Start_train: 8 Evaluation: 8 0.7192982456140351 4 Start_train: 9 Evaluation: 9 0.6403508771929824 4 Start_train: 10 Evaluation: 10 0.5657894736842105 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6710526315789473 4 Start_train: 13 Evaluation: 13 0.6228070175438597 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.618421052631579 4 Start_train: 16 Evaluation: 16 0.5701754385964912 4 Start_train: 17 Evaluation: 17 0.6403508771929824 4 Start_train: 18 Evaluation: 18 0.5570175438596491 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.7192982456140351 4 Start_train: 21 Evaluation: 21 0.6929824561403509 4 Start_train: 22 Evaluation: 22 0.5921052631578947 4 Start_train: 23 Evaluation: 23 0.5657894736842105 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.6008771929824561 4 Start_train: 26 Evaluation: 26 0.6666666666666666 4 Start_train: 27 Evaluation: 27 0.5570175438596491 4 Start_train: 28 Evaluation: 28 0.5570175438596491 4 Start_train: 29 Evaluation: 29 0.6885964912280702 4 Start_train: 30 Evaluation: 30 0.7105263157894737 4 Start_train: 31 Evaluation: 31 0.5570175438596491 4 Start_train: 32 Evaluation: 32 0.7149122807017544 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5570175438596491 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.5570175438596491 4 Start_train: 37 Evaluation: 37 0.5570175438596491 4 Start_train: 38 Evaluation: 38 0.6754385964912281 4 Start_train: 39 Evaluation: 39 0.5570175438596491 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.6622807017543859 4 Start_train: 42 Evaluation: 42 0.6535087719298246 4 Start_train: 43 Evaluation: 43 0.7017543859649122 4 Start_train: 44 Evaluation: 44 0.6271929824561403 4 Start_train: 45 Evaluation: 45 0.6228070175438597 4 Start_train: 46 Evaluation: 46 0.5570175438596491 4 Start_train: 47 Evaluation: 47 0.7149122807017544 4 Start_train: 48 Evaluation: 48 0.6578947368421053 4 Start_train: 49 Evaluation: 49 0.6754385964912281 4\"\n",
        "c7 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.7017543859649122 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.6271929824561403 4 Start_train: 8 Evaluation: 8 0.6052631578947368 4 Start_train: 9 Evaluation: 9 0.6491228070175439 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6535087719298246 4 Start_train: 13 Evaluation: 13 0.6140350877192983 4 Start_train: 14 Evaluation: 14 0.6491228070175439 4 Start_train: 15 Evaluation: 15 0.5570175438596491 4 Start_train: 16 Evaluation: 16 0.7456140350877193 4 Start_train: 17 Evaluation: 17 0.7368421052631579 4 Start_train: 18 Evaluation: 18 0.5657894736842105 4 Start_train: 19 Evaluation: 19 0.6140350877192983 4 Start_train: 20 Evaluation: 20 0.6578947368421053 4 Start_train: 21 Evaluation: 21 0.5570175438596491 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.5570175438596491 4 Start_train: 24 Evaluation: 24 0.5745614035087719 4 Start_train: 25 Evaluation: 25 0.5614035087719298 4 Start_train: 26 Evaluation: 26 0.6929824561403509 4 Start_train: 27 Evaluation: 27 0.6842105263157895 4 Start_train: 28 Evaluation: 28 0.6096491228070176 4 Start_train: 29 Evaluation: 29 0.7631578947368421 4 Start_train: 30 Evaluation: 30 0.6271929824561403 4 Start_train: 31 Evaluation: 31 0.6008771929824561 4 Start_train: 32 Evaluation: 32 0.706140350877193 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5921052631578947 4 Start_train: 35 Evaluation: 35 0.706140350877193 4 Start_train: 36 Evaluation: 36 0.5921052631578947 4 Start_train: 37 Evaluation: 37 0.6666666666666666 4 Start_train: 38 Evaluation: 38 0.5921052631578947 4 Start_train: 39 Evaluation: 39 0.6096491228070176 4 Start_train: 40 Evaluation: 40 0.6622807017543859 4 Start_train: 41 Evaluation: 41 0.5964912280701754 4 Start_train: 42 Evaluation: 42 0.7192982456140351 4 Start_train: 43 Evaluation: 43 0.5614035087719298 4 Start_train: 44 Evaluation: 44 0.6008771929824561 4 Start_train: 45 Evaluation: 45 0.5964912280701754 4 Start_train: 46 Evaluation: 46 0.7236842105263158 4 Start_train: 47 Evaluation: 47 0.5877192982456141 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.5570175438596491 4\"\n",
        "c8 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.7017543859649122 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.6271929824561403 4 Start_train: 8 Evaluation: 8 0.6052631578947368 4 Start_train: 9 Evaluation: 9 0.6491228070175439 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6535087719298246 4 Start_train: 13 Evaluation: 13 0.6140350877192983 4 Start_train: 14 Evaluation: 14 0.6491228070175439 4 Start_train: 15 Evaluation: 15 0.5570175438596491 4 Start_train: 16 Evaluation: 16 0.7456140350877193 4 Start_train: 17 Evaluation: 17 0.7368421052631579 4 Start_train: 18 Evaluation: 18 0.5657894736842105 4 Start_train: 19 Evaluation: 19 0.6140350877192983 4 Start_train: 20 Evaluation: 20 0.6578947368421053 4 Start_train: 21 Evaluation: 21 0.5570175438596491 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.5570175438596491 4 Start_train: 24 Evaluation: 24 0.5745614035087719 4 Start_train: 25 Evaluation: 25 0.5614035087719298 4 Start_train: 26 Evaluation: 26 0.6929824561403509 4 Start_train: 27 Evaluation: 27 0.6842105263157895 4 Start_train: 28 Evaluation: 28 0.6096491228070176 4 Start_train: 29 Evaluation: 29 0.7631578947368421 4 Start_train: 30 Evaluation: 30 0.6271929824561403 4 Start_train: 31 Evaluation: 31 0.6008771929824561 4 Start_train: 32 Evaluation: 32 0.706140350877193 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5921052631578947 4 Start_train: 35 Evaluation: 35 0.706140350877193 4 Start_train: 36 Evaluation: 36 0.5921052631578947 4 Start_train: 37 Evaluation: 37 0.6666666666666666 4 Start_train: 38 Evaluation: 38 0.5921052631578947 4 Start_train: 39 Evaluation: 39 0.6096491228070176 4 Start_train: 40 Evaluation: 40 0.6622807017543859 4 Start_train: 41 Evaluation: 41 0.5964912280701754 4 Start_train: 42 Evaluation: 42 0.7192982456140351 4 Start_train: 43 Evaluation: 43 0.5614035087719298 4 Start_train: 44 Evaluation: 44 0.6008771929824561 4 Start_train: 45 Evaluation: 45 0.5964912280701754 4 Start_train: 46 Evaluation: 46 0.7236842105263158 4 Start_train: 47 Evaluation: 47 0.5877192982456141 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.5570175438596491 4\"\n",
        "c9 = \"Start_train: 0 Evaluation: 0 0.631578947368421 4 Start_train: 1 Evaluation: 1 0.7412280701754386 4 Start_train: 2 Evaluation: 2 0.5175438596491229 4 Start_train: 3 Evaluation: 3 0.6710526315789473 4 Start_train: 4 Evaluation: 4 0.6929824561403509 4 Start_train: 5 Evaluation: 5 0.6666666666666666 4 Start_train: 6 Evaluation: 6 0.7324561403508771 4 Start_train: 7 Evaluation: 7 0.5570175438596491 4 Start_train: 8 Evaluation: 8 0.5789473684210527 4 Start_train: 9 Evaluation: 9 0.5570175438596491 4 Start_train: 10 Evaluation: 10 0.7236842105263158 4 Start_train: 11 Evaluation: 11 0.6973684210526315 4 Start_train: 12 Evaluation: 12 0.6666666666666666 4 Start_train: 13 Evaluation: 13 0.5789473684210527 4 Start_train: 14 Evaluation: 14 0.6666666666666666 4 Start_train: 15 Evaluation: 15 0.7017543859649122 4 Start_train: 16 Evaluation: 16 0.6929824561403509 4 Start_train: 17 Evaluation: 17 0.5570175438596491 4 Start_train: 18 Evaluation: 18 0.706140350877193 4 Start_train: 19 Evaluation: 19 0.6666666666666666 4 Start_train: 20 Evaluation: 20 0.7280701754385965 4 Start_train: 21 Evaluation: 21 0.5833333333333334 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.6973684210526315 4 Start_train: 24 Evaluation: 24 0.6710526315789473 4 Start_train: 25 Evaluation: 25 0.5570175438596491 4 Start_train: 26 Evaluation: 26 0.6359649122807017 4 Start_train: 27 Evaluation: 27 0.5833333333333334 4 Start_train: 28 Evaluation: 28 0.5570175438596491 4 Start_train: 29 Evaluation: 29 0.6491228070175439 4 Start_train: 30 Evaluation: 30 0.7105263157894737 4 Start_train: 31 Evaluation: 31 0.6885964912280702 4 Start_train: 32 Evaluation: 32 0.5964912280701754 4 Start_train: 33 Evaluation: 33 0.5701754385964912 4 Start_train: 34 Evaluation: 34 0.706140350877193 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.6666666666666666 4 Start_train: 37 Evaluation: 37 0.7280701754385965 4 Start_train: 38 Evaluation: 38 0.5614035087719298 4 Start_train: 39 Evaluation: 39 0.5614035087719298 4 Start_train: 40 Evaluation: 40 0.6535087719298246 4 Start_train: 41 Evaluation: 41 0.5570175438596491 4 Start_train: 42 Evaluation: 42 0.5570175438596491 4 Start_train: 43 Evaluation: 43 0.5570175438596491 4 Start_train: 44 Evaluation: 44 0.6447368421052632 4 Start_train: 45 Evaluation: 45 0.5789473684210527 4 Start_train: 46 Evaluation: 46 0.706140350877193 4 Start_train: 47 Evaluation: 47 0.618421052631579 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.6666666666666666 4\"\n",
        "c10 = \"Start_train: 0 Evaluation: 0 0.5701754385964912 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.6622807017543859 4 Start_train: 6 Evaluation: 6 0.5833333333333334 4 Start_train: 7 Evaluation: 7 0.45614035087719296 4 Start_train: 8 Evaluation: 8 0.5745614035087719 4 Start_train: 9 Evaluation: 9 0.6447368421052632 4 Start_train: 10 Evaluation: 10 0.6798245614035088 4 Start_train: 11 Evaluation: 11 0.618421052631579 4 Start_train: 12 Evaluation: 12 0.7149122807017544 4 Start_train: 13 Evaluation: 13 0.6271929824561403 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.631578947368421 4 Start_train: 16 Evaluation: 16 0.6403508771929824 4 Start_train: 17 Evaluation: 17 0.6929824561403509 4 Start_train: 18 Evaluation: 18 0.6008771929824561 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.7236842105263158 4 Start_train: 21 Evaluation: 21 0.6228070175438597 4 Start_train: 22 Evaluation: 22 0.5657894736842105 4 Start_train: 23 Evaluation: 23 0.6140350877192983 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.706140350877193 4 Start_train: 26 Evaluation: 26 0.5614035087719298 4 Start_train: 27 Evaluation: 27 0.5614035087719298 4 Start_train: 28 Evaluation: 28 0.5614035087719298 4 Start_train: 29 Evaluation: 29 0.706140350877193 4 Start_train: 30 Evaluation: 30 0.5570175438596491 4 Start_train: 31 Evaluation: 31 0.7236842105263158 4 Start_train: 32 Evaluation: 32 0.7236842105263158 4 Start_train: 33 Evaluation: 33 0.6359649122807017 4 Start_train: 34 Evaluation: 34 0.5789473684210527 4 Start_train: 35 Evaluation: 35 0.5745614035087719 4 Start_train: 36 Evaluation: 36 0.5745614035087719 4 Start_train: 37 Evaluation: 37 0.5745614035087719 4 Start_train: 38 Evaluation: 38 0.7236842105263158 4 Start_train: 39 Evaluation: 39 0.5570175438596491 4 Start_train: 40 Evaluation: 40 0.618421052631579 4 Start_train: 41 Evaluation: 41 0.7192982456140351 4 Start_train: 42 Evaluation: 42 0.5570175438596491 4 Start_train: 43 Evaluation: 43 0.5570175438596491 4 Start_train: 44 Evaluation: 44 0.7017543859649122 4 Start_train: 45 Evaluation: 45 0.7236842105263158 4 Start_train: 46 Evaluation: 46 0.5921052631578947 4 Start_train: 47 Evaluation: 47 0.6271929824561403 4 Start_train: 48 Evaluation: 48 0.7105263157894737 4 Start_train: 49 Evaluation: 49 0.7280701754385965 4\"\n",
        "\n",
        "results_raw = [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10]\n",
        "results =[process_strings(i) for i in results_raw]"
      ],
      "metadata": {
        "id": "bCke1gWWjwwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averages = []\n",
        "\n",
        "#get the average\n",
        "for r in range(0, len(results[0])):\n",
        "    num = 0\n",
        "    for c in range(0, len(results)):\n",
        "        num += results[c][r]\n",
        "    num = num/len(results)\n",
        "    averages.append(num)\n",
        "print(averages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPsBBL8nnV5z",
        "outputId": "d272a97b-58d6-4add-d1c5-cf9e0bec6f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5986842105263157, 0.6228070175438595, 0.5978070175438596, 0.5982456140350877, 0.5798245614035087, 0.594298245614035, 0.5903508771929823, 0.575, 0.6035087719298247, 0.619298245614035, 0.606140350877193, 0.5947368421052631, 0.631578947368421, 0.5942982456140351, 0.6074561403508772, 0.6166666666666666, 0.6241228070175439, 0.6605263157894736, 0.6092105263157894, 0.6030701754385964, 0.6587719298245613, 0.6219298245614034, 0.6043859649122807, 0.6298245614035087, 0.5912280701754385, 0.6017543859649123, 0.6307017543859649, 0.606578947368421, 0.6083333333333332, 0.6807017543859649, 0.631578947368421, 0.6171052631578948, 0.6359649122807017, 0.581140350877193, 0.6065789473684211, 0.6188596491228069, 0.5969298245614035, 0.624561403508772, 0.6171052631578947, 0.5956140350877192, 0.6109649122807017, 0.6223684210526316, 0.6083333333333333, 0.6210526315789473, 0.643859649122807, 0.6236842105263158, 0.6324561403508772, 0.6236842105263157, 0.6013157894736841, 0.6399122807017544]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotted below is the best result of the Learner model trained with MAML, after having seen 64 datapoints once ie. let the model see 64 datapoints and test on the validation data."
      ],
      "metadata": {
        "id": "Mkg9uEkOs9IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Experimentation: how fast does our meta-trained model learn?"
      ],
      "metadata": {
        "id": "Asexwgu0ktCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input: - the model you want to test\n",
        "#       - the dictionary in which you want to save the results\n",
        "def experiment(model):\n",
        "    results_experiment = {\n",
        "    'accuracy_training': [],\n",
        "    'loss': [],\n",
        "    'accuracy_validation': []\n",
        "}\n",
        "    for t in range(10):\n",
        "        print(t)\n",
        "        model = model\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        training_score = []\n",
        "        loss_score = []\n",
        "        validation_score = []\n",
        "\n",
        "        for e in range(5):\n",
        "            for i, data in enumerate(train_loader_sexist):\n",
        "                model.train()\n",
        "\n",
        "                running_loss = 0\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                inputs, labels = data\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optim.Adam(model.parameters(), lr=0.01).zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = model(inputs)\n",
        "                #Get the loss on the batch\n",
        "                targets = labels.to(torch.float32)\n",
        "                outputs = outputs.to(torch.float32)\n",
        "                \n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                #gets the gradient on the batch for each parameter\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "                #take size lr step in gradient direction\n",
        "                optimizer.step()\n",
        "\n",
        "                #calculate the f1_score\n",
        "                f1_score_train = f1_score(targets.detach().numpy(), torch.round(outputs).detach().numpy())\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                \n",
        "                #validation\n",
        "                model.eval()\n",
        "\n",
        "                #set metrics to 0\n",
        "                total_f1_score_val = 0\n",
        "                count_val = 0\n",
        "\n",
        "                for i_val, data_val in enumerate(val_loader_sexist):\n",
        "                    inputs_val, labels_val = data_val\n",
        "\n",
        "                    # forward + backward + optimize\n",
        "                    outputs_val = model(inputs_val)\n",
        "\n",
        "                    targets_val = labels_val.to(torch.float32)\n",
        "                    outputs_val = outputs_val.to(torch.float32)\n",
        "\n",
        "                    total_f1_score_val += f1_score(targets_val.detach().numpy(), torch.round(outputs_val).detach().numpy())\n",
        "                    count_val += 1\n",
        "                \n",
        "                training_score.append(f1_score_train)\n",
        "                validation_score.append(total_f1_score_val/count_val)\n",
        "                loss_score.append(running_loss/targets.shape[0])\n",
        "            \n",
        "                #set the metrics to 0\n",
        "                running_loss = 0\n",
        "\n",
        "        results_experiment['accuracy_training'].append(training_score)\n",
        "        results_experiment['accuracy_validation'].append(validation_score)\n",
        "        results_experiment['loss'].append(loss_score)\n",
        "\n",
        "    return results_experiment"
      ],
      "metadata": {
        "id": "NZNOXi0suMzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = torch.load(\"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state\")['embedding.weight']\n",
        "\n",
        "#fill the args dictionary with the required parameters\n",
        "args_model = {\n",
        "    'hidden_dim' : 32, #number of hidden dim\n",
        "    'vocab_size' : embedding_matrix.shape[0], #number of rows in word embedding matrix\n",
        "    'input_size' : 50,\n",
        "    'output_dim' : 1, #number of classes you're predicting\n",
        "    'embedding_matrix' : embedding_matrix, #the embedding matrix you've built in dataset constructor\n",
        "    'drp' : 0.2, #dropout layer for forward LSTM\n",
        "    'requires_grad': False,\n",
        "}\n",
        "\n",
        "#create the MAML model\n",
        "maml_trained_model = BiLSTM(args_model)\n",
        "maml_trained_model.load_state_dict(torch.load(\"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF2ruL4gypdB",
        "outputId": "64c1886f-2803-4d49-f238-c08081871638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_experiment = experiment(maml_trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPmfvqtkyrfs",
        "outputId": "ffceb4b0-e92c-447b-ec31-ec0ec5143581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the average\n",
        "results_avg_maml_bilstm = {\n",
        "    'accuracy_training': [],\n",
        "    'loss': [],\n",
        "    'accuracy_validation': []\n",
        "}\n",
        "\n",
        "#get the averages during the training phases\n",
        "for k,v in results_experiment.items():\n",
        "    score_raw = results_experiment[k]\n",
        "    score_avg = []\n",
        "    for r in range(0, len(score_raw[0])):\n",
        "        avg = 0\n",
        "        for i in range(0, len(score_raw)):\n",
        "            avg += score_raw[i][r]\n",
        "        score_avg.append(avg/len(score_raw))\n",
        "    results_avg_maml_bilstm[k] = score_avg"
      ],
      "metadata": {
        "id": "_h0MCwrPyvoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_results_vanilla = np.array(results_avg_vanilla_bilstm['accuracy_validation']).flatten()\n",
        "y1 = np.array(results_avg_vanilla_bilstm['accuracy_validation']).flatten()\n",
        "y2 = np.array(results_avg_maml_bilstm['accuracy_validation']).flatten()\n",
        "x = [i*16 for i in range(len(array_results_vanilla))]"
      ],
      "metadata": {
        "id": "edMT4UKJBlA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x, y1, label='MAML-augmented Bi-LSTM')\n",
        "plt.plot(x, y2, label='Vanilla Bi-LSTM')\n",
        "plt.legend()\n",
        "plt.xlabel('Training Sample Size')\n",
        "plt.ylabel('F1 score')"
      ],
      "metadata": {
        "id": "ln-mXj_Iy4_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}